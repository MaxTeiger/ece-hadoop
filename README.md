
# Syllabus

This lecture was originally designed for 10 weeks, but can be resized if needed.

## Introduction

Apache Hadoop is evolving as the Big Data platform on top of which multiple building
blocks are being developed. By the end of the course, students will be familiar to this
large and growing ecosystem. We start to learn how to composed distributed programs using
the MapReduce pattern before using higher level processing languages.
We firstly cover the 3 bricks of Apache Hadoop Framework: the filesystem (HDFS),
the processing pattern and its API (MR), and the scheduler / load balancer (YARN).
We then cover the complementary tasks of the data analyst and learn how to leverage
some of the most popular softwares that are used on top of the Hadoop platform.
Finally, we will explore the field of real-time processing through Spark Framework.

## Requirements

This course is best suited for student familiar to Programming and database systems.
Knowledge in Java and SQL is required. Being confortable with Linux, Git and Maven is a plus.
Prior knowledge of Distributed Systems, Apache Hadoop or NoSQL is not required.

## Outline

1.  The Hadoop ecosystem
2.  HDFS and MapReduce
3.  MapReduce and Yarn
4.  Pig
5.  Hive
6.  ETL (Extract, Transform, Load)
7.  HBase
8.  Spark
9.  Kafka
10. Preparation to Final Exam

## Bibliography/webography

No book is required. A course material will be provided (presentation slides),
and students will learn how-to retrieve up-to-date information
from internet (wikis, articles, blogs, source code).
